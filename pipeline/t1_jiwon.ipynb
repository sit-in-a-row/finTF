{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from sub_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_metrics(sector_info):\n",
    "    \"\"\"섹터 정보에서 Carhart 4 factor 관련 주요 지표 추출\"\"\"\n",
    "    if sector_info is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'market_beta': sector_info.get('market_beta', 0),\n",
    "        'size_factor': sector_info.get('size_factor', 0),\n",
    "        'value_factor': sector_info.get('value_factor', 0),\n",
    "        'momentum_factor': sector_info.get('momentum_factor', 0)\n",
    "    }\n",
    "\n",
    "def extract_sentiment_score(df):\n",
    "    \"\"\"감성분석 결과를 numerical score로 변환\"\"\"\n",
    "    def get_score(result):\n",
    "        if isinstance(result, dict):\n",
    "            # 딕셔너리에서 감성 결과 추출 (예: result.get('sentiment') 등)\n",
    "            sentiment = result.get('sentiment', 'neutral')  # 적절한 키로 수정\n",
    "        else:\n",
    "            sentiment = result\n",
    "            \n",
    "        score_mapping = {\n",
    "            'positive': 1.0,\n",
    "            'neutral': 0.0,\n",
    "            'negative': -1.0\n",
    "        }\n",
    "        return score_mapping.get(sentiment, 0.0)\n",
    "    \n",
    "    df['sentiment_score'] = df['SA_result'].apply(get_score)\n",
    "    return df\n",
    "\n",
    "def filter_by_percentile_and_label(df, label, percentile):\n",
    "    \"\"\"특정 감성의 상위/하위 percentile에 해당하는 뉴스 필터링\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # label에 따라 필터링\n",
    "    if label == 'positive':\n",
    "        filtered_df = df[df['SA_result'] == 'positive']\n",
    "        return filtered_df.nlargest(int(len(filtered_df) * percentile/100), 'sentiment_score')\n",
    "    else:  # negative\n",
    "        filtered_df = df[df['SA_result'] == 'negative']\n",
    "        return filtered_df.nsmallest(int(len(filtered_df) * percentile/100), 'sentiment_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestmentReportGenerator:\n",
    "    \"\"\"Investment report generation class with GPT integration\"\"\"\n",
    "    \n",
    "    MARKDOWN_INSTRUCTION = \"\"\"\n",
    "    응답은 반드시 markdown 문법에 따라 작성되어야 합니다.\n",
    "    ** 보고서에는 반드시 주어진 정보에 대한 분석이 필요합니다 **\n",
    "    \"\"\"\n",
    "\n",
    "    ANALYST_BASE_PROMPT = \"\"\"\n",
    "    당신은 증권회사에 고용된 {role}입니다.\n",
    "    주식투자의 관점에서 주어진 정보들을 요약하고, 이에 대한 의견을 알려주세요.\n",
    "    {additional_instructions}\n",
    "    {markdown_instruction}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ticker: str, year: str, quarter: str):\n",
    "        \"\"\"\n",
    "        Initialize the report generator.\n",
    "        \n",
    "        Args:\n",
    "            ticker: Stock ticker symbol\n",
    "            year: Target year\n",
    "            quarter: Target quarter (Q1-Q4)\n",
    "        \"\"\"\n",
    "        self.tickers = tickers\n",
    "        self.year = year\n",
    "        self.quarter = quarter\n",
    "        self.prompts = self._initialize_prompts()\n",
    "        self.responses = {ticker: {} for ticker in tickers}\n",
    "        self.report_data = {}\n",
    "        self.start_date, self.end_date = self._get_date_range()\n",
    "\n",
    "    def _initialize_prompts(self) -> Dict[str, str]:\n",
    "        \"\"\"Initialize system prompts with templated format\"\"\"\n",
    "        return {\n",
    "    \"financial_system\": self.ANALYST_BASE_PROMPT.format(\n",
    "        role=\"재무전문가\",\n",
    "        additional_instructions=\"보고서 근거 기반 의견 제시\",\n",
    "        markdown_instruction=self.MARKDOWN_INSTRUCTION\n",
    "    ),\n",
    "    \"financial_prompt\": \"\",\n",
    "\n",
    "    \"intl_macro_system\": self.ANALYST_BASE_PROMPT.format(\n",
    "        role=\"국제관계전문가\",\n",
    "        additional_instructions=\"국가별 금리, GDP, 인플레이션 등 거시경제 정보 분석\",\n",
    "        markdown_instruction=self.MARKDOWN_INSTRUCTION\n",
    "    ),\n",
    "    \"intl_macro_prompt\": \"\",\n",
    "\n",
    "    \"sector_system\": \"\"\"증권사 경제전문가로서 투자 관점에서 정보 분석 및 의견 제시\n",
    "        # 필수 포함 사항\n",
    "        - 섹터별 성과와 동향 분석\n",
    "        - 투자 매력도 평가 (근거 제시)\n",
    "        - 차트 패턴 분석 및 기술적 시사점\n",
    "        \"\"\" + self.MARKDOWN_INSTRUCTION,\n",
    "    \"sector_prompt\": \"\",\n",
    "\n",
    "    \"final_system\": \"\"\"증권사 애널리스트팀장으로서 전략적 포트폴리오 제안서 작성\n",
    "        # 필수 섹션 (각 3000토큰 이내)\n",
    "        1. 거시경제 분석\n",
    "        - 글로벌 동향 (GDP/물가/금리)\n",
    "        - 주요 리스크와 전망\n",
    "\n",
    "        2. 섹터 분석 \n",
    "        - 성과/동향/매력도\n",
    "        - 기술적 분석 시사점\n",
    "\n",
    "        3. 종목 분석 (각 1000토큰 이내)\n",
    "        - 재무분석 및 주요 지표\n",
    "        - 투자의견과 목표가\n",
    "\n",
    "        4. 포트폴리오 전략\n",
    "        - 자산배분 및 비중조정\n",
    "        - 위험관리 방안\n",
    "\n",
    "        5. 투자포인트/리스크\n",
    "        - 핵심 포인트 3개\n",
    "        - 리스크 요인과 대응\n",
    "\n",
    "        요구사항:\n",
    "        - 구체적 데이터/근거 제시\n",
    "        - 실행 가능한 전략 수립\n",
    "        \"\"\" + self.MARKDOWN_INSTRUCTION,\n",
    "    \"final_prompt\": \"\"\n",
    "}\n",
    "\n",
    "    def _get_date_range(self) -> tuple:\n",
    "        \"\"\"Get start and end dates for the given quarter\"\"\"\n",
    "        quarter_months = {\n",
    "            'Q1': ('01', '03'),\n",
    "            'Q2': ('04', '06'),\n",
    "            'Q3': ('07', '09'),\n",
    "            'Q4': ('10', '12')\n",
    "        }\n",
    "        \n",
    "        if self.quarter in quarter_months:\n",
    "            start_month, end_month = quarter_months[self.quarter]\n",
    "            start_date = f\"{self.year}{start_month}01\"\n",
    "            end_date = f\"{self.year}{end_month}{'30' if end_month in ['06', '09'] else '31'}\"\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid quarter: {self.quarter}\")\n",
    "\n",
    "    def analyze_financial_data(self, ticker: str) -> str:\n",
    "        \"\"\"Analyze financial statements and generate report\"\"\"\n",
    "        try:\n",
    "            fin_statement = get_raw_fin_statement_info(ticker, self.year, self.quarter)\n",
    "            fin_statement_dict = fin_statement.to_dict() if fin_statement is not None else {}\n",
    "        except Exception:\n",
    "            fin_statement_dict = {}\n",
    "\n",
    "        try:\n",
    "            fin_ratio = fin_statement_info(ticker, self.year, self.quarter)\n",
    "            fin_ratio_dict = fin_ratio.to_dict('records')[0] if fin_ratio is not None and not fin_ratio.empty else {}\n",
    "        except Exception:\n",
    "            fin_ratio_dict = {}\n",
    "\n",
    "        try:\n",
    "            fin_report = reports_info(ticker, self.year, self.quarter)\n",
    "            report_content = fin_report['1. 요약재무정보.csv'][0][4:-4] if not fin_report.empty else \"정보 없음\"\n",
    "        except Exception:\n",
    "            report_content = \"정보 없음\"\n",
    "        \n",
    "        prompt_data = {\n",
    "            \"재무제표\": fin_statement_dict,\n",
    "            \"주요 재무 비율\": fin_ratio_dict,\n",
    "            \"재무보고서\": report_content\n",
    "        }\n",
    "        \n",
    "        self.prompts[\"financial_prompt\"] = \"\\n\".join(f\"{k}: {v}\" for k, v in prompt_data.items())\n",
    "        return to_GPT(self.prompts[\"financial_system\"], self.prompts[\"financial_prompt\"])\n",
    "\n",
    "    def analyze_international_macro(self) -> str:\n",
    "        \"\"\"Analyze international news and macroeconomic data\"\"\"\n",
    "        try:\n",
    "            intl_news = intl_news_info(self.year, self.start_date, self.end_date)\n",
    "            news_titles = list(intl_news['news_title']) if intl_news is not None and not intl_news.empty else []\n",
    "        except Exception:\n",
    "            news_titles = []\n",
    "\n",
    "        try:\n",
    "            macro_data = macro_econ_info(self.year, self.start_date, self.end_date)\n",
    "        except Exception:\n",
    "            macro_data = \"거시경제 데이터 없음\"\n",
    "        \n",
    "        self.prompts[\"intl_macro_prompt\"] = \"\\n\".join([\n",
    "            f\"국제 뉴스 헤드라인: {news_titles}\",\n",
    "            f\"거시경제 관련 정보: {macro_data}\"\n",
    "        ])\n",
    "        \n",
    "        return to_GPT(self.prompts[\"intl_macro_system\"], self.prompts[\"intl_macro_prompt\"])\n",
    "\n",
    "    def analyze_sector_and_pattern(self, ticker: str) -> str:\n",
    "        \"\"\"Analyze sector trends and chart patterns\"\"\"\n",
    "        index_prices = {}\n",
    "        try:\n",
    "            sector_list = [s for s in os.listdir('../store_data/raw/market_data/sector') \n",
    "                          if '코스피' not in s]\n",
    "        except Exception:\n",
    "            sector_list = []\n",
    "        \n",
    "        # Collect sector data\n",
    "        for sector in sector_list:\n",
    "            try:\n",
    "                index_price = index_price_info(sector, self.start_date, self.end_date)\n",
    "                if index_price is not None and not index_price.empty:\n",
    "                    index_price = index_price[['Close', 'Transaction_Val', 'Market_Cap', 'RSI_14']]\n",
    "                    index_prices[sector] = index_price.T.to_dict()\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Collect sector analysis\n",
    "        sector_infos = {}\n",
    "        for sector in sector_list:\n",
    "            try:\n",
    "                sector_analysis = sector_analysis_info(sector, self.year, self.quarter)\n",
    "                if sector_analysis is not None:\n",
    "                    sector_infos[sector] = extract_key_metrics(sector_analysis)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            pattern_data = pattern_info(ticker, self.end_date.replace('-', ''))\n",
    "            pattern_dict = pattern_data.to_dict('records') if pattern_data is not None and not pattern_data.empty else None\n",
    "        except Exception:\n",
    "            pattern_dict = None\n",
    "        \n",
    "        self.prompts[\"sector_prompt\"] = \"\\n\".join([\n",
    "            f\"섹터별 가격 정보: {index_prices}\",\n",
    "            f\"섹터별 carhart 4 factor 분석: {sector_infos}\",\n",
    "            f\"차트 패턴 분석 결과: {pattern_dict}\"\n",
    "        ])\n",
    "        \n",
    "        return to_GPT(self.prompts[\"sector_system\"], self.prompts[\"sector_prompt\"])\n",
    "\n",
    "    def analyze_stocks(self):\n",
    "        \"\"\"Execute analysis for all stocks\"\"\"\n",
    "        # Only analyze macro once\n",
    "        macro_response = self.analyze_international_macro()\n",
    "        self.responses[\"international_macro\"] = macro_response\n",
    "        \n",
    "        for ticker in self.tickers:\n",
    "            print(f\"\\n=== {ticker} 분석 중... ===\")\n",
    "            try:\n",
    "                self.responses[ticker].update({\n",
    "                    \"financial\": self.analyze_financial_data(ticker),\n",
    "                    \"sector_pattern_analysis\": self.analyze_sector_and_pattern(ticker)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"{ticker} 분석 중 오류 발생: {e}\")\n",
    "\n",
    "    def generate_final_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive final report\"\"\"\n",
    "        combined_prompt = []\n",
    "        \n",
    "        for ticker in self.tickers:\n",
    "            try:\n",
    "                # Process news\n",
    "                corp_news_df = corp_rel_news_info(ticker, self.year, self.start_date, self.end_date)\n",
    "                news_summary = self._process_news(corp_news_df)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"{ticker}: 뉴스 데이터 파일이 없습니다.\")\n",
    "                news_summary = {'Positive': [], 'Negative': []}\n",
    "            except Exception as e:\n",
    "                print(f\"{ticker}: 뉴스 처리 중 오류 발생 - {e}\")\n",
    "                news_summary = {'Positive': [], 'Negative': []}\n",
    "            \n",
    "            try:\n",
    "                # Get stock price\n",
    "                stock_price = stock_price_info(ticker, self.start_date, self.end_date)\n",
    "                price_dict = stock_price.to_dict() if stock_price is not None and not stock_price.empty else None\n",
    "            except Exception:\n",
    "                price_dict = None\n",
    "            \n",
    "            # Add stock analysis\n",
    "            combined_prompt.extend([\n",
    "                f\"\\n=== {ticker} 종목 분석 ===\",\n",
    "                f\"재무제표 및 재무 비율 분석: {self._get_response_content(self.responses[ticker], 'financial')}\",\n",
    "                f\"종목 관련 뉴스: {news_summary}\",\n",
    "                f\"주가 정보: {price_dict}\",\n",
    "                f\"섹터 분석: {self._get_response_content(self.responses[ticker], 'sector_pattern_analysis')}\"\n",
    "            ])\n",
    "\n",
    "        # Add macro analysis\n",
    "        combined_prompt.extend([\n",
    "            \"\\n=== 거시경제 분석 ===\",\n",
    "            self._get_response_content(self.responses, \"international_macro\")\n",
    "        ])\n",
    "\n",
    "        self.prompts[\"final_prompt\"] = \"\\n\".join(combined_prompt)\n",
    "        final_response = to_GPT(self.prompts[\"final_system\"], self.prompts[\"final_prompt\"])\n",
    "        self.responses[\"final\"] = final_response\n",
    "        return final_response\n",
    "\n",
    "    def _process_news(self, corp_news_df) -> Dict[str, list]:\n",
    "        \"\"\"Process corporate news and extract sentiment\"\"\"\n",
    "        news_summary = {'Positive': [], 'Negative': []}\n",
    "        \n",
    "        if corp_news_df is not None and not corp_news_df.empty:\n",
    "            try:\n",
    "                # 증권 카테고리 필터링\n",
    "                corp_news_df = corp_news_df[corp_news_df['news_category'].str.contains('증권', na=False)]\n",
    "                \n",
    "                if not corp_news_df.empty:\n",
    "                    # SA 결과 및 감성 점수 추출\n",
    "                    corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x: \n",
    "                        get_SA_result(x) if pd.notna(x) else None)\n",
    "                    \n",
    "                    # None이나 NaN이 아닌 행만 감성 점수 추출\n",
    "                    valid_news = corp_news_df.dropna(subset=['SA_result'])\n",
    "                    if not valid_news.empty:\n",
    "                        valid_news = extract_sentiment_score(valid_news)\n",
    "                        \n",
    "                        for sentiment in ['positive', 'negative']:\n",
    "                            try:\n",
    "                                news = filter_by_percentile_and_label(valid_news, sentiment, 20)\n",
    "                                if not news.empty:\n",
    "                                    news_summary[sentiment.capitalize()] = list(news['news_title'])\n",
    "                            except Exception:\n",
    "                                continue\n",
    "            except Exception as e:\n",
    "                print(f\"뉴스 처리 중 오류 발생: {e}\")\n",
    "        \n",
    "        return news_summary\n",
    "\n",
    "    def _get_response_content(self, response_dict: Dict, key: str) -> str:\n",
    "        \"\"\"Safely extract content from GPT response\"\"\"\n",
    "        try:\n",
    "            return response_dict.get(key, {}).get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "        except (AttributeError, IndexError):\n",
    "            return ''\n",
    "\n",
    "    def print_report(self) -> None:\n",
    "        \"\"\"Print final report to stdout\"\"\"\n",
    "        if not self.responses.get(\"final\"):\n",
    "            self.generate_final_report()\n",
    "        \n",
    "        report_content = self.responses[\"final\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(\"\\n=== 최종 종합 보고서 ===\")\n",
    "        print(report_content)\n",
    "        print(\"\\n=== 보고서 끝 ===\\n\")\n",
    "\n",
    "    def save_report(self) -> None:\n",
    "        \"\"\"Save final report to Notion DB\"\"\"\n",
    "        page_title = f\"{year}_{quarter}_analyst_rp\"\n",
    "        print(f\"{page_title} 보고서를 노션 DB에 저장합니다...\")\n",
    "        to_DB('t_1', \n",
    "                page_title, \n",
    "                f\"{self.start_date}_{self.end_date}\", \n",
    "                self.responses[\"final\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        )\n",
    "\n",
    "    def _save_to_notion(self, page_title: str) -> None:\n",
    "        \"\"\"Save report to Notion database\"\"\"\n",
    "        print(f\"{page_title} 보고서를 노션 DB에 저장합니다...\")\n",
    "        to_DB('t_1', \n",
    "                page_title, \n",
    "                f\"{self.start_date}_{self.end_date}\", \n",
    "                self.responses[\"final\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK에 대해 Inflation Rate 정보를 찾을 수 없습니다. | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/FRED/UK/Inflation Rate/2022/2022_Inflation Rate.csv'\n",
      "\n",
      "=== 033660 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/033660/_033660_재무제표 ().csv'\n",
      "/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/market_data/price/033660/2022.10/2022.10_033660.csv 파일을 찾을 수 없습니다.\n",
      "/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/market_data/price/033660/2022.11/2022.11_033660.csv 파일을 찾을 수 없습니다.\n",
      "/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/market_data/price/033660/2022.12/2022.12_033660.csv 파일을 찾을 수 없습니다.\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/033660/_033660_재무제표 ().csv'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/033660/_033660_재무제표 ().csv'\n",
      "033660의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "033660의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 011420 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/011420/_011420_재무제표 ().csv'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/011420/_011420_재무제표 ().csv'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/011420/_011420_재무제표 ().csv'\n",
      "011420의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "011420의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 007810 분석 중... ===\n",
      "\n",
      "=== 267850 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/267850/_267850_재무제표 ().csv'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/267850/_267850_재무제표 ().csv'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/267850/_267850_재무제표 ().csv'\n",
      "267850의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "267850의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 092220 분석 중... ===\n",
      "\n",
      "=== 001740 분석 중... ===\n",
      "\n",
      "=== 195870 분석 중... ===\n",
      "\n",
      "=== 008770 분석 중... ===\n",
      "\n",
      "=== 003160 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/003160/_003160_재무제표 ().csv'\n",
      "003160의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "003160의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 000990 분석 중... ===\n",
      "\n",
      "=== 020000 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/020000/_020000_재무제표 ().csv'\n",
      "020000의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "020000의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 023800 분석 중... ===\n",
      "\n",
      "=== 039130 분석 중... ===\n",
      "\n",
      "=== 264900 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/264900'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/264900'\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/264900'\n",
      "264900의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "264900의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 093050 분석 중... ===\n",
      "\n",
      "=== 000480 분석 중... ===\n",
      "\n",
      "=== 077500 분석 중... ===\n",
      "\n",
      "=== 011070 분석 중... ===\n",
      "\n",
      "=== 029460 분석 중... ===\n",
      "\n",
      "=== 031430 분석 중... ===\n",
      "\n",
      "=== 004370 분석 중... ===\n",
      "\n",
      "=== 035720 분석 중... ===\n",
      "재무제표를 불러오는 과정에서 오류가 발생했습니다 | [Errno 2] No such file or directory: '/Users/gamjawon/finTF/pipeline/sub_func/get_info/../../../store_data/raw/opendart/store_financial_statement/035720/_035720_재무제표 ().csv'\n",
      "035720의 이전 년도 재무 데이터를 불러올 수 없습니다.\n",
      "035720의 fin_statement_info 정보를 확인할 수 없습니다.\n",
      "\n",
      "=== 025560 분석 중... ===\n",
      "033660: 뉴스 데이터 파일이 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n",
      "/var/folders/y7/7hr2z6vd3fb6nbtgn0gwlm840000gn/T/ipykernel_26510/3681078760.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corp_news_df['SA_result'] = corp_news_df['news_title'].apply(lambda x:\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 435495 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m analyzer\u001b[38;5;241m.\u001b[39manalyze_stocks()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 최종 보고서 생성\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m final_report \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_final_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 보고서 출력\u001b[39;00m\n\u001b[1;32m     62\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mprint_report()\n",
      "Cell \u001b[0;32mIn[27], line 251\u001b[0m, in \u001b[0;36mInvestmentReportGenerator.generate_final_report\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m combined_prompt\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 거시경제 분석 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response_content(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternational_macro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m ])\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(combined_prompt)\n\u001b[0;32m--> 251\u001b[0m final_response \u001b[38;5;241m=\u001b[39m \u001b[43mto_GPT\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_system\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m final_response\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_response\n",
      "File \u001b[0;32m~/finTF/pipeline/sub_func/to_gpt.py:29\u001b[0m, in \u001b[0;36mto_GPT\u001b[0;34m(system, prompt)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_GPT\u001b[39m(system, prompt):\n\u001b[0;32m---> 29\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/finTF/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/finTF/venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/finTF/venv/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/finTF/venv/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/finTF/venv/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 435495 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "json_file_path = \"/Users/gamjawon/finTF/pipeline/notion_page_ids.json\"\n",
    "\n",
    "# JSON 파일 읽기 함수\n",
    "def read_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "    \n",
    "# JSON 파일 수정\n",
    "def write_json(file_path, new_data):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(new_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "# 'pf_selection_agent' 섹션에서 종목 코드 추출\n",
    "def get_tickers_from_json(agent_type, title):\n",
    "   data = read_json(json_file_path)\n",
    "   if agent_type in data and title in data[agent_type]:\n",
    "       page_id = data[agent_type][title]\n",
    "       content = get_all_text_from_page(page_id)\n",
    "       \n",
    "       try:\n",
    "           # final_portfolio 부분 추출\n",
    "           start = content.find(\"'final_portfolio'\")\n",
    "           end = content.find(\"'corp_analysis_report'\")\n",
    "           portfolio_str = content[start:end].strip()\n",
    "           \n",
    "           # 종목코드만 추출\n",
    "           import re\n",
    "           tickers = re.findall(r\"'(\\d{6})'\", portfolio_str)\n",
    "           return list(set(tickers))  # 중복 제거\n",
    "           \n",
    "       except Exception as e:\n",
    "           print(f\"Error: {e}\")\n",
    "           return []\n",
    "   return []\n",
    "\n",
    "# 예시로 'pf_selection_agent'의 '2022_Q4_init_pf'에서 종목 코드 추출\n",
    "if __name__ == \"__main__\":\n",
    "    # 'pf_selection_agent'에서 '2022_Q4_init_pf' 종목 코드 가져오기\n",
    "    tickers = get_tickers_from_json('pf_selection_agent', '2022_Q4_init_pf')\n",
    "\n",
    "    # 연도 및 분기 설정\n",
    "    year = \"2022\"\n",
    "    quarter = \"Q4\"\n",
    "\n",
    "    # 분석기 객체 생성\n",
    "    analyzer = InvestmentReportGenerator(tickers, year, quarter)\n",
    "\n",
    "    # 분석 실행\n",
    "    analyzer.analyze_stocks()\n",
    "\n",
    "    # 최종 보고서 생성\n",
    "    final_report = analyzer.generate_final_report()\n",
    "\n",
    "    # 보고서 출력\n",
    "    analyzer.print_report()\n",
    "\n",
    "    # 노션에 저장\n",
    "    analyzer.save_report()  # 이 부분은 주석 처리하여 노션에 저장하려면 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_Q4_init_pf 보고서를 노션 DB에 저장합니다...\n",
      "Error: 404, {'object': 'error', 'status': 404, 'code': 'object_not_found', 'message': 'Could not find database with ID: 3827dd2d-b394-44a3-9c11-762153d30714. Make sure the relevant pages and databases are shared with your integration.', 'request_id': '814a740c-47da-405e-a3d4-11f82b0f6cde'}\n",
      "오류 발생: 404 Client Error: Not Found for url: https://api.notion.com/v1/pages\n"
     ]
    }
   ],
   "source": [
    "analyzer.save_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioManagerReportGenerator:\n",
    "    def __init__(self, ticker: str, year: str, quarter: str):\n",
    "        self.ticker = ticker\n",
    "        self.year = year\n",
    "        self.quarter = quarter\n",
    "        self.prompts = self._initialize_prompts()\n",
    "        self.responses = {}\n",
    "        self.report_data = {}\n",
    "        self.analyst_report = None\n",
    "        self.current_portfolio = None\n",
    "        \n",
    "    def _initialize_prompts(self) -> Dict[str, str]:\n",
    "        \"\"\"프롬프트 초기화\"\"\"\n",
    "        return {\n",
    "            \"portfolio_manager_system\": \"\"\"당신은 자산운용사의 수석 포트폴리오 매니저입니다. \n",
    "애널리스트팀의 리서치 보고서를 검토하여 전체 포트폴리오 운용 전략을 제시하세요.\n",
    "\n",
    "분석 핵심:\n",
    "- 종목/섹터 비중 분산\n",
    "- 투자매력도와 리스크\n",
    "- 거시경제 환경\n",
    "- 위험/수익 특성\n",
    "\n",
    "보고서 구조:\n",
    "\n",
    "# 1. 포트폴리오 현황\n",
    "- 전체 구성과 섹터 비중\n",
    "- 주요 위험/수익 지표\n",
    "- 현 포트폴리오 강약점\n",
    "\n",
    "# 2. 시장 환경\n",
    "- 거시경제/섹터 전망\n",
    "- 핵심 리스크/기회 요인\n",
    "\n",
    "# 3. 종목별 전략\n",
    "각 종목:\n",
    "- 현재/적정 비중\n",
    "- 투자매력도\n",
    "- 주요 모멘텀/리스크\n",
    "- 비중 조절 방향\n",
    "\n",
    "# 4. 포트폴리오 조정\n",
    "- 운용 방향성\n",
    "- 종목별 비중 조정(확대/축소/편입/편출)\n",
    "- 섹터 전략\n",
    "\n",
    "# 5. 리스크 관리\n",
    "- 포트폴리오/종목별 관리\n",
    "- 손절/이익실현 기준\n",
    "- 모니터링 지표\n",
    "\n",
    "요구사항:\n",
    "1. 구체적 수치/근거 포함\n",
    "2. 현황/전망 균형있게 서술\n",
    "3. 전체 수익/위험 고려\n",
    "4. 실행 가능한 전략 제시\n",
    "\n",
    "응답은 markdown 형식으로 작성\"\"\",\n",
    "            \"portfolio_manager_prompt\": \"\"\n",
    "        }\n",
    "        \n",
    "    def set_analyst_report(self, report: str) -> None:\n",
    "        \"\"\"애널리스트 보고서 설정\"\"\"\n",
    "        self.analyst_report = report\n",
    "        \n",
    "    def set_current_portfolio(self, portfolio: Dict) -> None:\n",
    "        \"\"\"현재 포트폴리오 설정\"\"\"\n",
    "        self.current_portfolio = portfolio\n",
    "        \n",
    "    def generate_portfolio_report(self) -> str:\n",
    "        \"\"\"포트폴리오 매니저 보고서 생성\"\"\"\n",
    "        if self.analyst_report is None or self.current_portfolio is None:\n",
    "            raise ValueError(\"Analyst report and current portfolio must be set before generating report\")\n",
    "            \n",
    "        self.prompts[\"portfolio_manager_prompt\"] = f\"현재 포트폴리오 구성: {self.current_portfolio}\\n\"\n",
    "        self.prompts[\"portfolio_manager_prompt\"] += f\"애널리스트 보고서: {self.analyst_report}\"\n",
    "        \n",
    "        response = to_GPT(self.prompts[\"portfolio_manager_system\"], \n",
    "                         self.prompts[\"portfolio_manager_prompt\"])\n",
    "        self.responses[\"portfolio_manager\"] = response\n",
    "        return response\n",
    "        \n",
    "    def save_report(self) -> None:\n",
    "        \"\"\"노션 DB에 포트폴리오 매니저 보고서 저장\"\"\"\n",
    "        if not self.responses.get(\"portfolio_manager\"):\n",
    "            self.generate_portfolio_report()\n",
    "        \n",
    "        page_title = f\"{self.year}_{self.quarter}_{self.ticker}_portfolio_manager_report\"\n",
    "\n",
    "        print(f\"{page_title} 보고서를 노션 DB에 저장합니다...\")\n",
    "        to_DB('t_1', \n",
    "            page_title, \n",
    "            f\"{self.quarter}_{self.year}\", \n",
    "            self.responses[\"portfolio_manager\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_portfolio():\n",
    "    data = read_json(json_file_path)\n",
    "    agent_type = 'pf_selection_agent'\n",
    "    title = '2022_Q4_init_pf'\n",
    "    # 특정 agent_type과 title에 해당하는 종목 코드 추출\n",
    "    if agent_type in data and title in data[agent_type]:\n",
    "        portfolio = data[agent_type][title]\n",
    "        return portfolio\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 2. 종합 포트폴리오 매니저 보고서 생성\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 티커는 대표 종목으로 설정하거나 \"MULTI\"와 같은 식별자 사용\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     portfolio_manager \u001b[38;5;241m=\u001b[39m PortfolioManagerReportGenerator(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtickers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_외_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tickers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m종목\u001b[39m\u001b[38;5;124m\"\u001b[39m, year, quarter)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 애널리스트 보고서와 현재 포트폴리오 설정\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     portfolio_manager\u001b[38;5;241m.\u001b[39mset_analyst_report(final_report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "# 2. 종합 포트폴리오 매니저 보고서 생성\n",
    "    # 티커는 대표 종목으로 설정하거나 \"MULTI\"와 같은 식별자 사용\n",
    "    portfolio_manager = PortfolioManagerReportGenerator(f\"{tickers[0]}_외_{len(tickers)-1}종목\", year, quarter)\n",
    "    \n",
    "    # 애널리스트 보고서와 현재 포트폴리오 설정\n",
    "    portfolio_manager.set_analyst_report(final_report[\"choices\"][0][\"message\"][\"content\"])\n",
    "    portfolio_manager.set_current_portfolio(get_current_portfolio())\n",
    "    \n",
    "    # 보고서 생성\n",
    "    portfolio_report = portfolio_manager.generate_portfolio_report()\n",
    "    \n",
    "    # 보고서 출력\n",
    "    print(\"\\n=== 포트폴리오 매니저 종합 보고서 ===\")\n",
    "    print(portfolio_report[\"choices\"][0][\"message\"][\"content\"])\n",
    "    print(\"=== 보고서 끝 ===\")\n",
    "    \n",
    "    # 선택적으로 노션에 저장\n",
    "    portfolio_manager.save_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraderReportGenerator:\n",
    "    def __init__(self, tickers: List[str], year: str, quarter: str):\n",
    "        self.tickers = tickers\n",
    "        self.year = year\n",
    "        self.quarter = quarter\n",
    "        self.prompts = self._initialize_prompts()\n",
    "        self.responses = {ticker: {} for ticker in tickers}\n",
    "        self.report_data = {}\n",
    "        self.start_date, self.end_date = self._get_date_range()\n",
    "        self.price_data = {}\n",
    "        self.analyst_reports = {}\n",
    "        self.pm_reports = {}\n",
    "        self.price_predictions = {}\n",
    "        \n",
    "    def _initialize_prompts(self) -> Dict[str, str]:\n",
    "        \"\"\"프롬프트 초기화\"\"\"\n",
    "        return {\n",
    "            \"trader_system\": \"\"\"당신은 증권사의 트레이더입니다. 여러 종목의 데이터와 보고서들을 분석하여 핵심적인 매매 의견을 제시하되, 출력은 3만~4만 토큰 이내로 제한해야 합니다.\n",
    "\n",
    "        응답 형식:\n",
    "        # 시장 전반 분석 (1-2 문단)\n",
    "        - 주요 시장 동향\n",
    "        - 핵심 매매 전략\n",
    "\n",
    "        # 주요 관심 종목 분석 (상위 5-7개 종목)\n",
    "        각 종목별로:\n",
    "        - 현재가/예측가 핵심 동향\n",
    "        - 매매 방향과 가격대\n",
    "        - 주요 리스크와 손절가\n",
    "\n",
    "        # 기타 종목 매매 요약 (1-2문단)\n",
    "        - 매수/매도 종목 구분\n",
    "        - 핵심 진입/청산 전략\n",
    "\n",
    "        모든 분석은 반드시 제공된 데이터(가격, 예측, 애널리스트/PM 보고서)에 기반해야 합니다.\n",
    "        응답은 markdown 형식으로 작성하되, 핵심 정보 위주로 간단명료하게 작성하세요.\"\"\",\n",
    "            \"trader_prompt\": \"\"\n",
    "        }\n",
    "\n",
    "    def _get_date_range(self) -> tuple:\n",
    "        \"\"\"분기에 해당하는 시작일과 종료일 반환\"\"\"\n",
    "        quarter_months = {\n",
    "            'Q1': ('01', '03'),\n",
    "            'Q2': ('04', '06'),\n",
    "            'Q3': ('07', '09'),\n",
    "            'Q4': ('10', '12')\n",
    "        }\n",
    "        \n",
    "        if self.quarter in quarter_months:\n",
    "            start_month, end_month = quarter_months[self.quarter]\n",
    "            start_date = f\"{self.year}{start_month}01\"\n",
    "            end_date = f\"{self.year}{end_month}{'30' if end_month in ['06', '09'] else '31'}\"\n",
    "            return start_date, end_date\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid quarter: {self.quarter}\")\n",
    "\n",
    "    def set_price_data(self) -> None:\n",
    "        \"\"\"주가 데이터 설정\"\"\"\n",
    "        try:\n",
    "            for ticker in self.tickers:\n",
    "                self.price_data[ticker] = stock_price_info(ticker, self.start_date, self.end_date)\n",
    "        except Exception as e:\n",
    "            print(f\"가격 데이터 설정 중 오류 발생: {str(e)}\")\n",
    "\n",
    "\n",
    "    def set_analyst_report(self, report: str) -> None:\n",
    "        \"\"\"애널리스트 보고서 설정\"\"\"\n",
    "        self.analyst_report = report\n",
    "\n",
    "    def set_pm_report(self, report: str) -> None:\n",
    "        \"\"\"포트폴리오 매니저 보고서 설정\"\"\"\n",
    "        self.pm_report = report\n",
    "\n",
    "    def get_price_prediction(self) -> None:\n",
    "        \"\"\"GRU 모델을 사용한 가격 예측\"\"\"\n",
    "        try:\n",
    "            self.price_predictions = predict_multiple_prices(\n",
    "                self.tickers,\n",
    "                self.start_date,\n",
    "                self.end_date\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"가격 예측 모델 실행 중 오류 발생: {str(e)}\")\n",
    "\n",
    "    def generate_trader_report(self) -> str:\n",
    "        \"\"\"트레이더 보고서 생성\"\"\"\n",
    "        price_data_str = {ticker: data.to_dict() if hasattr(data, 'to_dict') else data \n",
    "                        for ticker, data in self.price_data.items()} if self.price_data else {}\n",
    "        \n",
    "        self.prompts[\"trader_prompt\"] = \"\\n\".join([\n",
    "            f\"전체 주가 데이터: {price_data_str}\",\n",
    "            f\"전체 종목 가격 예측: {self.price_predictions or {}}\",\n",
    "            f\"애널리스트 종합 보고서: {self.analyst_report or '정보 없음'}\",\n",
    "            f\"포트폴리오 매니저 종합 보고서: {self.pm_report or '정보 없음'}\"\n",
    "        ])\n",
    "\n",
    "        response = to_GPT(self.prompts[\"trader_system\"], self.prompts[\"trader_prompt\"])\n",
    "        self.responses[\"trader\"] = response\n",
    "        return response\n",
    "\n",
    "    def save_report(self) -> None:\n",
    "        \"\"\"노션 DB에 트레이더 보고서 저장\"\"\"\n",
    "        if not self.responses.get(\"trader\"):\n",
    "            self.generate_trader_report()\n",
    "\n",
    "        page_title = f\"{self.year}_{self.quarter}_전체종목_trader_report\"\n",
    "   \n",
    "        db_info = get_all_page_ids_from_database('t_1')\n",
    "        \n",
    "        to_DB('t_1', \n",
    "                page_title, \n",
    "                f\"{self.quarter}_{self.year}\", \n",
    "                self.responses[\"trader\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            )\n",
    "        '''\n",
    "        if page_title not in list(db_info.keys()):\n",
    "            print(f\"{page_title} 보고서를 노션 DB에 저장합니다...\")\n",
    "            to_DB('t_1', \n",
    "                page_title, \n",
    "                f\"{self.quarter}_{self.year}\", \n",
    "                self.responses[\"trader\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{page_title} 보고서가 이미 존재합니다.\")\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['005930']\n",
      "로드 데이터 함수 시작\n",
      "[DEBUG] 새로운 데이터 로드 시도\n",
      "[DEBUG] 연도: 2020, 월: 10\n",
      "[DEBUG] 매핑된 분기: Q4\n",
      "[DEBUG] 재무제표 데이터 로드: True\n",
      "\n",
      "[DEBUG] 외국인 보유 비중 데이터 로드 시도\n",
      "[DEBUG] 외국인 보유 비중 데이터 로드 성공\n",
      "[DEBUG] 데이터 병합 완료\n",
      "[DEBUG] 최종 데이터 shape: (61, 5)\n",
      "[DEBUG] 학습 데이터 통계:\n",
      "X_train shape: (46, 15, 3)\n",
      "y_train shape: (46,)\n",
      "X_train 값 범위: 0.0 ~ 1.0\n",
      "y_train 값 범위: 0.0 ~ 1.0\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gamjawon/finTF/venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - loss: 0.0856 - mae: 0.3562 - val_loss: 0.1916 - val_mae: 0.6095 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0495 - mae: 0.2611 - val_loss: 0.1485 - val_mae: 0.5330 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0288 - mae: 0.1940 - val_loss: 0.1099 - val_mae: 0.4540 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0174 - mae: 0.1487 - val_loss: 0.0801 - val_mae: 0.3829 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0092 - mae: 0.1075 - val_loss: 0.0584 - val_mae: 0.3209 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0079 - mae: 0.1054 - val_loss: 0.0426 - val_mae: 0.2666 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0087 - mae: 0.1151 - val_loss: 0.0334 - val_mae: 0.2296 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0090 - mae: 0.1167 - val_loss: 0.0283 - val_mae: 0.2068 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0098 - mae: 0.1218 - val_loss: 0.0270 - val_mae: 0.2025 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0079 - mae: 0.1082 - val_loss: 0.0271 - val_mae: 0.2047 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.1010 - val_loss: 0.0275 - val_mae: 0.2084 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0060 - mae: 0.0935 - val_loss: 0.0279 - val_mae: 0.2118 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0046 - mae: 0.0812 - val_loss: 0.0279 - val_mae: 0.2126 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0042 - mae: 0.0762 - val_loss: 0.0265 - val_mae: 0.2067 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - mae: 0.0812 - val_loss: 0.0237 - val_mae: 0.1936 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0043 - mae: 0.0793 - val_loss: 0.0207 - val_mae: 0.1781 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - mae: 0.0695 - val_loss: 0.0177 - val_mae: 0.1611 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0033 - mae: 0.0679 - val_loss: 0.0153 - val_mae: 0.1463 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - mae: 0.0635 - val_loss: 0.0135 - val_mae: 0.1348 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - mae: 0.0645 - val_loss: 0.0119 - val_mae: 0.1230 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - mae: 0.0612 - val_loss: 0.0106 - val_mae: 0.1137 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0027 - mae: 0.0595 - val_loss: 0.0095 - val_mae: 0.1040 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0026 - mae: 0.0592 - val_loss: 0.0082 - val_mae: 0.0918 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - mae: 0.0515 - val_loss: 0.0070 - val_mae: 0.0823 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - mae: 0.0500 - val_loss: 0.0060 - val_mae: 0.0782 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0476 - val_loss: 0.0052 - val_mae: 0.0755 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - mae: 0.0490 - val_loss: 0.0048 - val_mae: 0.0744 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0022 - mae: 0.0510 - val_loss: 0.0046 - val_mae: 0.0732 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - mae: 0.0485 - val_loss: 0.0045 - val_mae: 0.0718 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - mae: 0.0441 - val_loss: 0.0044 - val_mae: 0.0708 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020 - mae: 0.0462 - val_loss: 0.0040 - val_mae: 0.0704 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0433 - val_loss: 0.0038 - val_mae: 0.0697 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - mae: 0.0474 - val_loss: 0.0035 - val_mae: 0.0696 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0457 - val_loss: 0.0034 - val_mae: 0.0694 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0444 - val_loss: 0.0033 - val_mae: 0.0692 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017 - mae: 0.0457 - val_loss: 0.0033 - val_mae: 0.0698 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - mae: 0.0441 - val_loss: 0.0032 - val_mae: 0.0684 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017 - mae: 0.0425 - val_loss: 0.0032 - val_mae: 0.0674 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - mae: 0.0411 - val_loss: 0.0031 - val_mae: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0413 - val_loss: 0.0031 - val_mae: 0.0667 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0015 - mae: 0.0415 - val_loss: 0.0031 - val_mae: 0.0681 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - mae: 0.0446 - val_loss: 0.0031 - val_mae: 0.0697 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - mae: 0.0461 - val_loss: 0.0031 - val_mae: 0.0694 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - mae: 0.0457 - val_loss: 0.0031 - val_mae: 0.0685 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - mae: 0.0464 - val_loss: 0.0031 - val_mae: 0.0682 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0015 - mae: 0.0430 - val_loss: 0.0030 - val_mae: 0.0674 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0459 - val_loss: 0.0030 - val_mae: 0.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014 - mae: 0.0409 - val_loss: 0.0030 - val_mae: 0.0661 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - mae: 0.0469 - val_loss: 0.0030 - val_mae: 0.0659 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015 - mae: 0.0422 - val_loss: 0.0030 - val_mae: 0.0658 - learning_rate: 5.0000e-04\n",
      "\n",
      "[DEBUG] 모델 평가:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "검증 세트 MSE: 0.020269083952514595\n",
      "\n",
      "=== 트레이더 종합 보고서 ===\n",
      "```markdown\n",
      "# 시장 가격 분석\n",
      "- **현재 가격 동향**: 삼성전자의 현재 주가는 81,000원으로, 최근 몇 주 동안 상승세를 이어오고 있습니다. 특히, 최근 며칠 동안의 거래량이 증가하며 주가가 80,000원을 상회하는 모습을 보였습니다.\n",
      "- **주요 기술적 지표**:\n",
      "  - **MA_20**: 73,910원, 현재 주가는 이보다 높아 상승세가 지속되고 있습니다.\n",
      "  - **RSI_14**: 79.93, 과매수 구간에 진입하고 있어 조정 가능성이 있습니다.\n",
      "  - **Bollinger Bands**: 현재 주가는 상한선(BBU_20_2.0)인 79,609.79원을 초과하여 있어 가격의 조정이 있을 수 있습니다.\n",
      "- **가격 예측 모델 결과**: 머신러닝 모델에 따르면, 향후 주가는 78,656원에서 82,247원 사이로 예측되고 있습니다. 현재 주가가 이 범위의 상단에 가까워 향후 조정이 예상됩니다.\n",
      "\n",
      "# 투자의견 종합\n",
      "- **애널리스트 분석 요약**: 삼성전자는 현재 안정적인 성장성과 긍정적인 재무지표를 보유하고 있으며, 매수 추천이 이루어졌습니다. 반도체 수요 증가와 같은 긍정적인 모멘텀이 있는 반면, 과도한 포트폴리오 비중으로 인한 리스크도 언급되었습니다.\n",
      "- **포트폴리오 매니저 전략 요약**: 현재 포트폴리오에서 삼성전자의 비중을 45%에서 30%로 축소하고, 금융업 관련 ETF를 신규 편입하여 리스크를 분산할 것을 제안하고 있습니다.\n",
      "- **매매 실행 관점에서의 평가**: 삼성전자의 주가 상승세와 긍정적인 전망에도 불구하고, 과매수 신호와 높은 비중 리스크를 고려할 때 매도 전략이 필요합니다.\n",
      "\n",
      "# 매매 실행 계획\n",
      "- **매매 방향**: 매도\n",
      "- **진입/청산 가격대**: 목표 가격은 81,000원에서 82,000원 (주가가 82,000원에 도달 시 매도).\n",
      "- **주문 실행 전략**: 시장가 매도로 청산하며, 주가가 80,500원에 도달 시 매도 주문을 우선적으로 실행합니다.\n",
      "\n",
      "# 리스크 관리\n",
      "- **손절매 수준**: 주가가 79,000원으로 하락할 경우 손절매 설정.\n",
      "- **모니터링 포인트**: 실업률, 금리 인상 및 글로벌 경제 지표 변화에 대한 주시 필요. 특히, 미국의 금리 인상이 삼성전자의 주가에 미치는 영향 관찰.\n",
      "- **대응 계획**: 손절매에 도달할 경우 즉시 매도하며, 주가가 상승세를 보일 경우 추가 매수 기회 모색.\n",
      "```\n",
      "=== 보고서 끝 ===\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "# 2. 종합 포트폴리오 매니저 보고서 생성\n",
    "    # 티커는 대표 종목으로 설정하거나 \"MULTI\"와 같은 식별자 사용\n",
    "\n",
    "    trader = TraderReportGenerator(tickers, year, quarter)\n",
    "    \n",
    "    # 가격 데이터와 애널리스트 보고서와 현재 포트폴리오 설정\n",
    "    trader.set_price_data()\n",
    "    trader.set_analyst_report(final_report[\"choices\"][0][\"message\"][\"content\"])\n",
    "    trader.set_pm_report(portfolio_report[\"choices\"][0][\"message\"][\"content\"])\n",
    "    \n",
    "    # 보고서 생성\n",
    "    trader_report = trader.generate_trader_report()\n",
    "    \n",
    "    # 보고서 출력\n",
    "    print(\"\\n=== 트레이더 종합 보고서 ===\")\n",
    "    print(trader_report[\"choices\"][0][\"message\"][\"content\"])\n",
    "    print(\"=== 보고서 끝 ===\")\n",
    "    \n",
    "    # 선택적으로 노션에 저장\n",
    "    # trader.save_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재무제표 데이터:\n",
      "   Stock Price        PER       PBR       ROE  Profit Growth Rate (%)  \\\n",
      "0        81000  18.310946  1.752331  0.095699                21.47751   \n",
      "\n",
      "   CAGR (%)  \n",
      "0  0.214775  \n"
     ]
    }
   ],
   "source": [
    "fin_data = fin_statement_info(\"005930\", \"2020\", \"Q4\")\n",
    "print(\"재무제표 데이터:\")\n",
    "print(fin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sub_func import *\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import joblib\n",
    "from pykrx import stock\n",
    "\n",
    "def predict_multiple_prices(tickers: list, start_date: str, end_date: str, price_data_dict=None) -> dict:\n",
    "    predictions = {}\n",
    "    \n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data = load_stock_data(ticker, start_date, end_date)\n",
    "            if data is None:\n",
    "                continue\n",
    "                \n",
    "            # PER이 0인 경우 처리\n",
    "            if np.all(data['PER'] == 0):\n",
    "                print(\"[WARNING] PER이 모두 0입니다. 평균값으로 대체합니다.\")\n",
    "                data['PER'] = 15.0  # 일반적인 PER 평균값으로 대체\n",
    "            \n",
    "            # 스케일링\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(data[['close', 'high', 'PER', 'foreign_holding']])\n",
    "            data[['close', 'high', 'PER', 'foreign_holding']] = scaled_data\n",
    "            \n",
    "            # 시계열 데이터 준비\n",
    "            window_size = 15\n",
    "            X = []\n",
    "            y = []\n",
    "            for i in range(window_size, len(data)):\n",
    "                X.append(data[['high', 'PER', 'foreign_holding']].values[i-window_size:i])\n",
    "                y.append(data['close'].values[i])\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            # 모델 학습\n",
    "            model = create_and_train_model(X, y, ticker)\n",
    "            \n",
    "            # 예측 수행\n",
    "            last_sequence = X[-1:]\n",
    "            future_predictions = []\n",
    "            current_sequence = last_sequence.copy()\n",
    "            \n",
    "            for _ in range(20):\n",
    "                pred = model.predict(current_sequence, verbose=0)\n",
    "                future_predictions.append(float(pred[0, 0]))\n",
    "                \n",
    "                current_sequence = np.roll(current_sequence, -1, axis=1)\n",
    "                current_sequence[0, -1] = [pred[0, 0], data['PER'].iloc[-1], data['foreign_holding'].iloc[-1]]\n",
    "            \n",
    "            # 예측값 역변환\n",
    "            future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "            future_predictions = np.concatenate([future_predictions, np.zeros((len(future_predictions), 3))], axis=1)\n",
    "            future_predictions = scaler.inverse_transform(future_predictions)[:, 0]\n",
    "            \n",
    "            predictions[ticker] = {\n",
    "                'current_price': float(scaler.inverse_transform([[data['close'].iloc[-1], 0, 0, 0]])[0, 0]),\n",
    "                'predicted_prices': future_predictions.tolist(),\n",
    "                'prediction_dates': pd.date_range(\n",
    "                    start=pd.to_datetime(data['date'].iloc[-1]) + pd.Timedelta(days=1),\n",
    "                    periods=20\n",
    "                ).strftime('%Y-%m-%d').tolist()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ticker} 예측 중 오류 발생: {str(e)}\")\n",
    "            predictions[ticker] = None\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_price(ticker: str, start_date: str = None, end_date: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    GRU 모델을 사용하여 주가를 예측하는 함수\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): 주식 종목 코드\n",
    "        start_date (str): 예측 시작일 (YYYYMMDD 형식)\n",
    "        end_date (str): 예측 종료일 (YYYYMMDD 형식)\n",
    "    \n",
    "    Returns:\n",
    "        dict: 예측 결과를 담은 딕셔너리\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 모델 및 스케일러 경로 설정\n",
    "        MODEL_PATH = f'models/{ticker}_gru_model.h5'\n",
    "        SCALER_PATH = f'models/{ticker}_scaler.pkl'\n",
    "        \n",
    "        # 데이터 로드 \n",
    "        data = load_stock_data(ticker, start_date, end_date)  \n",
    "        \n",
    "        # 데이터 전처리\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data = data.sort_values('date')\n",
    "        \n",
    "        # 저장된 스케일러 로드 또는 새로 생성\n",
    "        if os.path.exists(SCALER_PATH):\n",
    "            scaler = joblib.load(SCALER_PATH)\n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            data[['close', 'high', 'PER', 'foreign_holding']] = scaler.fit_transform(\n",
    "                data[['close', 'high', 'PER', 'foreign_holding']]\n",
    "            )\n",
    "            joblib.dump(scaler, SCALER_PATH)\n",
    "        \n",
    "        # 시계열 데이터 준비\n",
    "        window_size = 15\n",
    "        X = []\n",
    "        for i in range(window_size, len(data)):\n",
    "            X.append(data[['high', 'PER', 'foreign_holding']].values[i-window_size:i])\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # 모델 로드 또는 새로 생성\n",
    "        if os.path.exists(MODEL_PATH):\n",
    "            model = load_model(MODEL_PATH)\n",
    "        else:\n",
    "            model = create_and_train_model(X, data['close'].values[window_size:], ticker)\n",
    "        \n",
    "        # 다음 분기 예측\n",
    "        future_predictions = []\n",
    "        last_sequence = X[-1:]\n",
    "        \n",
    "        # 다음 20일(약 한 달) 예측\n",
    "        for _ in range(20):\n",
    "            next_pred = model.predict(last_sequence)\n",
    "            future_predictions.append(next_pred[0, 0])\n",
    "            \n",
    "            # 다음 예측을 위한 시퀀스 업데이트\n",
    "            last_sequence = np.roll(last_sequence, -1, axis=1)\n",
    "            last_sequence[0, -1] = next_pred\n",
    "        \n",
    "        # 예측값 역변환\n",
    "        future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "        future_predictions = scaler.inverse_transform(\n",
    "            np.concatenate((future_predictions, np.zeros((future_predictions.shape[0], 3))), axis=1)\n",
    "        )[:, 0]\n",
    "        \n",
    "        # 결과 정리\n",
    "        result = {\n",
    "            'current_price': data['close'].iloc[-1],\n",
    "            'predicted_prices': future_predictions.tolist(),\n",
    "            'prediction_dates': pd.date_range(\n",
    "                start=data['date'].iloc[-1] + pd.Timedelta(days=1), \n",
    "                periods=20\n",
    "            ).strftime('%Y-%m-%d').tolist(),\n",
    "            'confidence_level': calculate_confidence_level(model, X, data['close'].values[window_size:])\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"예측 중 오류 발생: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_and_train_model(X_train, y_train, ticker):\n",
    "    \"\"\"GRU 모델 생성 및 학습\"\"\"\n",
    "    print(f\"[DEBUG] 학습 데이터 통계:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_train 값 범위: {np.min(X_train)} ~ {np.max(X_train)}\")\n",
    "    print(f\"y_train 값 범위: {np.min(y_train)} ~ {np.max(y_train)}\")\n",
    "\n",
    "    # 입력 데이터 검증\n",
    "    if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n",
    "        raise ValueError(\"입력 데이터에 NaN 또는 무한값이 포함되어 있습니다.\")\n",
    "\n",
    "    if np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n",
    "        raise ValueError(\"타겟 데이터에 NaN 또는 무한값이 포함되어 있습니다.\")\n",
    "\n",
    "    # 모델 구조\n",
    "    model = Sequential([\n",
    "        GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # 컴파일\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='huber',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # 콜백\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 학습 결과 검증\n",
    "    print(\"\\n[DEBUG] 모델 평가:\")\n",
    "    val_predictions = model.predict(X_train[-int(len(X_train)*0.2):])\n",
    "    val_true = y_train[-int(len(y_train)*0.2):]\n",
    "    mse = np.mean((val_predictions - val_true) ** 2)\n",
    "    print(f\"검증 세트 MSE: {mse}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_confidence_level(model, X, y_true):\n",
    "    \"\"\"예측 신뢰도 계산\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    mse = np.mean((y_pred - y_true.reshape(-1, 1)) ** 2)\n",
    "    confidence = np.exp(-mse)  # 0~1 사이의 값으로 변환\n",
    "    return float(confidence)\n",
    "\n",
    "def load_stock_data(ticker: str, start_date: str, end_date: str, price_data=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주식 데이터를 로드하는 함수\n",
    "    price_data: TraderReportGenerator에서 이미 로드된 가격 데이터\n",
    "    \"\"\"\n",
    "    print(\"로드 데이터 함수 시작\")\n",
    "    try:\n",
    "        if price_data is not None:\n",
    "            print(\"[DEBUG] 기존 price_data 사용\")  # 디버깅\n",
    "            selected_data = pd.DataFrame()\n",
    "            selected_data['close'] = price_data['Close']\n",
    "            selected_data['high'] = price_data['High']\n",
    "            selected_data['date'] = price_data.index\n",
    "            selected_data = selected_data.reset_index(drop=True)\n",
    "        else:\n",
    "            print(\"[DEBUG] 새로운 데이터 로드 시도\")  # 디버깅\n",
    "            price_data = stock_price_info(ticker, start_date, end_date)\n",
    "            \n",
    "            if price_data is None:\n",
    "                print(f\"Warning: 가격 데이터를 가져올 수 없습니다: {ticker}\")\n",
    "                return None\n",
    "            \n",
    "            selected_data = pd.DataFrame()\n",
    "            selected_data['close'] = price_data['Close']\n",
    "            selected_data['high'] = price_data['High']\n",
    "            selected_data['date'] = price_data.index\n",
    "            selected_data = selected_data.reset_index(drop=True)\n",
    "            \n",
    "        # 연도와 분기 추출\n",
    "        year = start_date[:4]\n",
    "        month = start_date[4:6]\n",
    "        print(f\"[DEBUG] 연도: {year}, 월: {month}\")  # 디버깅\n",
    "        \n",
    "        # 분기 매핑\n",
    "        quarter_map = {\n",
    "            'Q1': ['01', '02', '03'],\n",
    "            'Q2': ['04', '05', '06'],\n",
    "            'Q3': ['07', '08', '09'],\n",
    "            'Q4': ['10', '11', '12']\n",
    "        }\n",
    "        \n",
    "        quarter = next(q for q, months in quarter_map.items() if month in months)\n",
    "        print(f\"[DEBUG] 매핑된 분기: {quarter}\")  # 디버깅\n",
    "        \n",
    "        # PER 데이터 가져오기\n",
    "        fin_data = fin_statement_info(ticker, year, quarter)\n",
    "        print(f\"[DEBUG] 재무제표 데이터 로드: {fin_data is not None}\")  # 디버깅\n",
    "        \n",
    "        if fin_data is not None:\n",
    "            per_value = fin_data['PER'].iloc[0]\n",
    "        else:\n",
    "            per_value = None\n",
    "            \n",
    "        # PER 컬럼 추가\n",
    "        selected_data['PER'] = per_value\n",
    "        \n",
    "        # 외국인 보유 비중 추가\n",
    "        try:\n",
    "            print(\"\\n[DEBUG] 외국인 보유 비중 데이터 로드 시도\")\n",
    "            foreign_data = stock.get_exhaustion_rates_of_foreign_investment(start_date, end_date, ticker)\n",
    "            print(f\"[DEBUG] 외국인 보유 비중 데이터 로드 성공\")\n",
    "            \n",
    "            # 데이터 병합을 위해 인덱스 처리\n",
    "            selected_data['date'] = pd.to_datetime(selected_data['date'])\n",
    "            foreign_data = foreign_data.reset_index()\n",
    "            foreign_data.columns = ['date' if col == '날짜' else col for col in foreign_data.columns]\n",
    "            foreign_data['date'] = pd.to_datetime(foreign_data['date'])\n",
    "            \n",
    "            # 날짜 기준으로 데이터 병합\n",
    "            selected_data = pd.merge(selected_data, \n",
    "                                   foreign_data[['date', '지분율']], \n",
    "                                   on='date', \n",
    "                                   how='left')\n",
    "            selected_data = selected_data.rename(columns={'지분율': 'foreign_holding'})\n",
    "            \n",
    "            print(\"[DEBUG] 데이터 병합 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] 외국인 보유 비중 로드 실패: {str(e)}\")\n",
    "            selected_data['foreign_holding'] = 0\n",
    "            \n",
    "        print(f\"[DEBUG] 최종 데이터 shape: {selected_data.shape}\")  # 디버깅\n",
    "        return selected_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"데이터 로드 중 오류 발생: {str(e)}\")\n",
    "        print(f\"[DEBUG] 오류 발생 위치 정보: {e.__traceback__.tb_lineno}\")  # 디버깅\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finTF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
