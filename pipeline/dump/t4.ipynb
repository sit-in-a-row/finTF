{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notion_client\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#ì‹œì‘ ì „ ì„¸íŒ…\n",
    "\n",
    "# pipeline ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../pipeline/sub_func\")))\n",
    "from to_gpt import to_GPT\n",
    "\n",
    "# API ë° DB ì„¤ì •\n",
    "def load_api_keys():\n",
    "    \"\"\"api_keys.yamlì—ì„œ API í‚¤ë¥¼ ë¡œë“œ\"\"\"\n",
    "    config_path = os.path.join(os.path.dirname(__file__), \"../config/api_keys.yaml\")\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "api_keys = load_api_keys()\n",
    "NOTION_API_KEY = api_keys[\"api_keys\"][\"NOTION_API_TOKEN\"]\n",
    "T1_DB_ID = api_keys[\"DB_IDs\"][\"t_1\"]\n",
    "T4_DB_ID = api_keys[\"DB_IDs\"][\"t_4\"]\n",
    "\n",
    "notion = notion_client.Client(auth=NOTION_API_KEY)\n",
    "\n",
    "def fetch_specific_notion_data(database_id, titles):\n",
    "    \"\"\"ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŠ¹ì • ì œëª©ì„ ê°€ì§„ í˜ì´ì§€ ëª©ë¡ì„ ê°€ì ¸ì˜´\"\"\"\n",
    "    response = notion.databases.query(database_id)\n",
    "    results = []\n",
    "\n",
    "    for page in response[\"results\"]:\n",
    "        title_property = page[\"properties\"].get(\"Title\", {}).get(\"title\", [])\n",
    "        if title_property:\n",
    "            title = title_property[0][\"text\"][\"content\"]\n",
    "            if title in titles:\n",
    "                results.append(page)\n",
    "\n",
    "    print(\"\\nğŸ” Fetched Notion Pages:\", results)  \n",
    "    return results\n",
    "\n",
    "def fetch_notion_page_content(page_id):\n",
    "    \"\"\"Notion í˜ì´ì§€ì˜ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    response = notion.blocks.children.list(block_id=page_id)\n",
    "    content = []\n",
    "\n",
    "    for block in response[\"results\"]:\n",
    "        if block[\"type\"] == \"paragraph\":\n",
    "            text_content = block[\"paragraph\"][\"rich_text\"]\n",
    "            if text_content:\n",
    "                content.append(text_content[0][\"plain_text\"])\n",
    "\n",
    "    full_text = \"\\n\".join(content)\n",
    "    print(\"\\nğŸ“œ Extracted Text from Notion:\\n\", full_text[:500])  \n",
    "    return full_text\n",
    "\n",
    "def extract_stocks_from_text(text):\n",
    "    \"\"\"Notion ë³¸ë¬¸ì—ì„œ ì¢…ëª© ì •ë³´ë¥¼ ì¶”ì¶œ\"\"\"\n",
    "    stock_dict = {}\n",
    "\n",
    "    pattern_table = r\"\\|\\s*(\\d{6})\\s*\\|\\s*(.*?)\\s*\\|\\s*([\\d.]+)\\s*\\|\\s*([\\d.]+)\\s*\\|\\s*([\\d.]+%)\\s*\\|\\s*([\\d.]+%)\\s*\\|\\s*([\\d,]+)\\s*\\|\\s*(.*?)\\s*\\|\"\n",
    "    matches_table = re.findall(pattern_table, text)\n",
    "\n",
    "    print(\"\\nDEBUG: Extracted Stocks (Before Saving)\")\n",
    "    \n",
    "    for match in matches_table:\n",
    "        stock_code, company_name, per, pbr, roe, growth, target_price, notes = match\n",
    "        # ë§¤ìˆ˜/ë§¤ë„ ì—¬ë¶€ íŒë³„\n",
    "        action = \"ë§¤ìˆ˜\" if \"ì €í‰ê°€\" in notes or \"ë°˜ë“±\" in notes else \"ë§¤ë„\"\n",
    "\n",
    "        stock_dict[stock_code] = {\n",
    "            \"ì¢…ëª©ëª…\": company_name.strip(),\n",
    "            \"PER\": float(per),\n",
    "            \"PBR\": float(pbr),\n",
    "            \"ROE\": roe,\n",
    "            \"ì´ìµ ì„±ì¥ë¥ \": growth,\n",
    "            \"ëª©í‘œê°€\": int(target_price.replace(\",\", \"\")),\n",
    "            \"íˆ¬ìí¬ì¸íŠ¸\": notes.strip(),\n",
    "            \"ë§¤ìˆ˜/ë§¤ë„\": action\n",
    "        }\n",
    "\n",
    "        print(f\"ğŸ“Œ ì¢…ëª© ì½”ë“œ: {stock_code}, ì¢…ëª©ëª…: {company_name.strip()}\")  # ë””ë²„ê¹…ìš©\n",
    "\n",
    "    return stock_dict\n",
    "\n",
    "\n",
    "# trader ì¤‘ì‹¬ ì½”ë“œ\n",
    "def generate_trade_log(all_stocks):\n",
    "    \"\"\"GPTë¥¼ ì´ìš©í•˜ì—¬ ê±°ë˜ ë¡œê·¸ ìƒì„± (ë§¤ìˆ˜/ë§¤ë„ í¬í•¨)\"\"\"\n",
    "    system_prompt = (\n",
    "        \"ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ ì‹œì¥ íŠ¸ë ˆì´ë”ì…ë‹ˆë‹¤. \"\n",
    "        \"ì£¼ì–´ì§„ ì¢…ëª© ì½”ë“œì— ëŒ€í•´ ê±°ë˜ ë¡œê·¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "    )\n",
    "\n",
    "    print(\"\\nDEBUG: ì¢…ëª© ì½”ë“œ ì‚¬ìš© (Before Sending to GPT)\")\n",
    "    for code, stock in all_stocks.items():\n",
    "        print(f\"ğŸ“Œ ì¢…ëª© ì½”ë“œ: {code}\")  # ì¢…ëª© ì½”ë“œë§Œ ì¶œë ¥\n",
    "\n",
    "    stock_list = \"\\n\".join([\n",
    "        f\"{code}: {stock['ë§¤ìˆ˜/ë§¤ë„']} | ëª©í‘œê°€ {stock.get('ëª©í‘œê°€', 'Unknown')}ì›, íˆ¬ìí¬ì¸íŠ¸ {stock.get('íˆ¬ìí¬ì¸íŠ¸', 'N/A')}\"\n",
    "        for code, stock in all_stocks.items()\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ë‹¤ìŒ ì¢…ëª©ë“¤ì— ëŒ€í•œ ê±°ë˜ ë¡œê·¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    í•´ë‹¹ ì¢…ëª©ì˜ ë§¤ìˆ˜/ë§¤ë„ ì „ëµì„ í¬í•¨í•˜ì—¬ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "    í˜•ì‹:\n",
    "    ì¢…ëª© ì½”ë“œ | ë§¤ìˆ˜/ë§¤ë„ | ê°€ê²© | íˆ¬ìí¬ì¸íŠ¸\n",
    "\n",
    "    ì¢…ëª© ë¦¬ìŠ¤íŠ¸:\n",
    "    {stock_list}\n",
    "    \n",
    "    ë˜í•œ ì¢…ëª©ë“¤ì— ëŒ€í•œ ë§¤ìˆ˜/ë§¤ë„ ì „ëµì´ ëë‚¬ë‹¤ë©´ íˆ¬ì ìˆœìœ„ì™€ ì£¼ì˜í•  ì  ë“±ì„ ë§ˆì§€ë§‰ì— í•œê¸€ë¡œ ì²¨ë¶€í•´ì£¼ì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nğŸ“ Sending trade log request to GPT...\")\n",
    "    print(f\"DEBUG: GPTë¡œ ì „ë‹¬í•˜ëŠ” ë°ì´í„°:\\n{stock_list}\")  # GPTì— ì „ë‹¬í•˜ëŠ” ë°ì´í„° í™•ì¸\n",
    "\n",
    "    response = to_GPT(system_prompt, prompt)\n",
    "\n",
    "    trade_log = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"\\nâœ… Generated Trade Log:\", trade_log)  # ìµœì¢… ì¶œë ¥ í™•ì¸\n",
    "    \n",
    "    return trade_log\n",
    "\n",
    "\n",
    "# analyst ì¤‘ì‹¬ ì½”ë“œ\n",
    "def fetch_news_data(ticker: str, year: int, quarter: int) -> pd.DataFrame:\n",
    "    base_path = f\"store_data/raw/crawling/corp_rel_news/{ticker}/{year}\"\n",
    "    quarter_months = {1: ['01', '02', '03'], 2: ['04', '05', '06'], 3: ['07', '08', '09'], 4: ['10', '11', '12']}\n",
    "    news_data = []\n",
    "\n",
    "    for month in quarter_months[quarter]:\n",
    "        month_path = os.path.join(base_path, month)\n",
    "        if os.path.exists(month_path):\n",
    "            for file in os.listdir(month_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(month_path, file)\n",
    "                    print(f\"ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ ì‹œë„: {file_path}\")\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, encoding='utf-8')  # CSV íŒŒì¼ ì½ê¸°\n",
    "                        \n",
    "                        # 1ï¸**íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸**\n",
    "                        print(f\"CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ({file_path}):\")\n",
    "                        print(df.head())  # CSV ë‚´ìš© í™•ì¸\n",
    "                        \n",
    "                        # 2ï¸**ì»¬ëŸ¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸**\n",
    "                        print(f\"CSV íŒŒì¼ì˜ ì»¬ëŸ¼ëª…: {df.columns.tolist()}\")\n",
    "\n",
    "                        df.columns = [col.strip() for col in df.columns]  # ê³µë°± ì œê±°\n",
    "                        \n",
    "                        # 3ï¸*í•„ìš”í•œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸**\n",
    "                        if not {'news_title', 'news_category', 'news_date'}.issubset(df.columns):\n",
    "                            print(f\"CSV íŒŒì¼ {file_path}ì— ì˜ˆìƒëœ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {df.columns}\")\n",
    "                            continue  # ì»¬ëŸ¼ì´ ë‹¤ë¥´ë©´ ìŠ¤í‚µ\n",
    "\n",
    "                        # 4ï¸**ë‚ ì§œ ë³€í™˜ì´ ì •ìƒì ìœ¼ë¡œ ë˜ëŠ”ì§€ í™•ì¸**\n",
    "                        df['news_date'] = pd.to_datetime(df['news_date'], format='%Y.%m.%d', errors='coerce')\n",
    "                        print(f\"ë³€í™˜ëœ ë‚ ì§œ ë°ì´í„° (ìµœì´ˆ 5ê°œ):\")\n",
    "                        print(df['news_date'].head())\n",
    "\n",
    "                        df = df.dropna(subset=['news_date'])  # ë³€í™˜ ì‹¤íŒ¨í•œ ë‚ ì§œ ì œê±°\n",
    "                        df.rename(columns={'news_title': 'headline'}, inplace=True)  # ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "                        news_data.append(df[['news_date', 'headline']])\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ CSV íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path} - {e}\")\n",
    "\n",
    "    if news_data:\n",
    "        news_df = pd.concat(news_data, ignore_index=True)\n",
    "        news_df = news_df.sort_values(by='news_date')\n",
    "        news_df.set_index('news_date', inplace=True)\n",
    "    else:\n",
    "        print(f\"âš ï¸ {ticker}ì— ëŒ€í•œ {year} Q{quarter} ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        news_df = pd.DataFrame(columns=['news_date', 'headline'])\n",
    "\n",
    "    return news_df\n",
    "\n",
    "\n",
    "def analyze_news_combined(all_stocks, year, quarter):\n",
    "    \"\"\"ëª¨ë“  ì¢…ëª©ì˜ ë‰´ìŠ¤ ë¶„ì„ì„ í•œê¸€ë¡œ ìš”ì•½í•˜ì—¬ 2000ì ì œí•œìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "    system_prompt = f\"\"\"\n",
    "    ë‹¹ì‹ ì€ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì´ë©°, ì£¼ì‹ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "    ë¶„ì„ ëŒ€ìƒ ê¸°ê°„ì€ ë°˜ë“œì‹œ **{year}ë…„ {quarter}ë¶„ê¸°**ì…ë‹ˆë‹¤. \n",
    "    ë‹¤ë¥¸ ì—°ë„ë‚˜ ë¶„ê¸°ì˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ë§ê³ , ì œê³µëœ ë°ì´í„°ë§Œ í™œìš©í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    stock_news = []\n",
    "    for ticker in all_stocks.keys():\n",
    "        news_df = fetch_news_data(ticker, year, quarter)\n",
    "\n",
    "        if news_df.empty or 'headline' not in news_df.columns:\n",
    "            print(f\"ğŸš¨ {ticker} ë‰´ìŠ¤ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŒ: news_df.empty = {news_df.empty}\")\n",
    "            stock_news.append(f\"**ì¢…ëª© ì½”ë“œ: {ticker}**\\ní˜„ì¬ í•´ë‹¹ ì¢…ëª©ê³¼ ê´€ë ¨ëœ ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(f\"âœ… {ticker} ë‰´ìŠ¤ ë¶„ì„ ì¤‘...\")\n",
    "            stock_news.append(f\"**ì¢…ëª© ì½”ë“œ: {ticker}**\\n\" + \"\\n\".join(news_df['headline'].tolist()))\n",
    "\n",
    "    news_report = \"\\n\\n\".join(stock_news)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ì•„ë˜ëŠ” {year}ë…„ {quarter}ë¶„ê¸° ë™ì•ˆ ê° ì¢…ëª©ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.\n",
    "    ë°˜ë“œì‹œ **{year}ë…„ {quarter}ë¶„ê¸°**ì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ë§Œ ë°˜ì˜í•˜ì—¬ í•œêµ­ì–´ë¡œ ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    **ë‹¤ë¥¸ ì—°ë„ë‚˜ ìµœì‹  ë°ì´í„°ë¥¼ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**\n",
    "    \n",
    "    {news_report}\n",
    "    \n",
    "    **ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ**:\n",
    "    \n",
    "    **í†µí•© ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸**\n",
    "    \n",
    "    {year}ë…„ {quarter}ë¶„ê¸° ê¸°ì¤€ìœ¼ë¡œ ì—¬ëŸ¬ ì£¼ìš” ì¢…ëª©ì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ì™€ ì‹œì¥ ë™í–¥ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    1. **ì‚¼ì„±ì „ì (005930)**  \n",
    "    - ìµœê·¼ ë°˜ë„ì²´ ì‹œì¥ ì „ë§ì´ ê¸ì •ì ì´ë©°, {year}ë…„ {quarter}ë¶„ê¸° ì‹¤ì ì´ ì˜ˆìƒë³´ë‹¤ ê°•ì„¸ë¥¼ ë³´ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŒ.\n",
    "    \n",
    "    2. **í˜„ëŒ€ì°¨ (005380)**  \n",
    "    - ì „ê¸°ì°¨ ì‚¬ì—… í™•ì¥ê³¼ ê¸€ë¡œë²Œ ê³µê¸‰ë§ ê°œì„ ì´ ì˜ˆìƒë˜ë©°, {year}ë…„ {quarter}ë¶„ê¸° ë™ì•ˆ ì§€ì†ì ì¸ ì„±ì¥ì„¸ë¥¼ ìœ ì§€ ì¤‘.\n",
    "\n",
    "    ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    response = to_GPT(system_prompt, prompt)\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"][:2000]\n",
    "\n",
    "\n",
    "# ë…¸ì…˜ì— ì €ì¥\n",
    "def save_to_notion(title, content, database_id, period):\n",
    "    \"\"\"Notion ë°ì´í„°ë² ì´ìŠ¤ì— ìƒˆë¡œìš´ í˜ì´ì§€ë¥¼ ì¶”ê°€í•˜ê³  ë³¸ë¬¸ì„ í•œê¸€ë¡œ ì±„ì›€\"\"\"\n",
    "\n",
    "    new_page = notion.pages.create(\n",
    "        parent={\"database_id\": database_id},\n",
    "        properties={\n",
    "            \"Title\": {\n",
    "                \"title\": [{\"text\": {\"content\": title}}]\n",
    "            },\n",
    "            \"Period\": {\n",
    "                \"rich_text\": [{\"text\": {\"content\": period}}]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    page_id = new_page[\"id\"]\n",
    "    notion.blocks.children.append(\n",
    "        block_id=page_id,\n",
    "        children=[\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [{\"type\": \"text\", \"text\": {\"content\": content[:2000]}}]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# ë©”ì¸\n",
    "def main(year, quarter):\n",
    "    period = f\"{year}_Q{quarter}\"  \n",
    "\n",
    "    pages = fetch_specific_notion_data(T1_DB_ID, [f\"{period}_analyst_rp\", f\"{period}_final_trader_report\"])\n",
    "    all_stocks = {}\n",
    "\n",
    "    for page in pages:\n",
    "        stocks = extract_stocks_from_text(fetch_notion_page_content(page[\"id\"]))\n",
    "        all_stocks.update(stocks)\n",
    "\n",
    "    trade_log = generate_trade_log(all_stocks)\n",
    "    save_to_notion(f\"{period}_trader_log\", trade_log, T4_DB_ID, period)\n",
    "\n",
    "    analyst_report = analyze_news_combined(all_stocks, year, quarter)\n",
    "    save_to_notion(f\"{period}_analyst_report\", analyst_report, T4_DB_ID, period)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(2022, 4) # ì˜ˆì‹œ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
