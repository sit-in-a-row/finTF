{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 articles to ./news_2/2024/11/20241109.json\n",
      "Crawling done for 20241109. Progress: 1.11%\n",
      "Saved 37 articles to ./news_2/2024/11/20241110.json\n",
      "Crawling done for 20241110. Progress: 2.22%\n",
      "Saved 193 articles to ./news_2/2024/11/20241111.json\n",
      "Crawling done for 20241111. Progress: 3.33%\n",
      "Saved 198 articles to ./news_2/2024/11/20241112.json\n",
      "Crawling done for 20241112. Progress: 4.44%\n",
      "Saved 283 articles to ./news_2/2024/11/20241113.json\n",
      "Crawling done for 20241113. Progress: 5.56%\n",
      "Saved 198 articles to ./news_2/2024/11/20241114.json\n",
      "Crawling done for 20241114. Progress: 6.67%\n",
      "Saved 188 articles to ./news_2/2024/11/20241115.json\n",
      "Crawling done for 20241115. Progress: 7.78%\n",
      "Saved 18 articles to ./news_2/2024/11/20241116.json\n",
      "Crawling done for 20241116. Progress: 8.89%\n",
      "Saved 49 articles to ./news_2/2024/11/20241117.json\n",
      "Crawling done for 20241117. Progress: 10.00%\n",
      "Saved 166 articles to ./news_2/2024/11/20241118.json\n",
      "Crawling done for 20241118. Progress: 11.11%\n",
      "Saved 151 articles to ./news_2/2024/11/20241119.json\n",
      "Crawling done for 20241119. Progress: 12.22%\n",
      "Saved 169 articles to ./news_2/2024/11/20241120.json\n",
      "Crawling done for 20241120. Progress: 13.33%\n",
      "Saved 132 articles to ./news_2/2024/11/20241121.json\n",
      "Crawling done for 20241121. Progress: 14.44%\n",
      "Saved 144 articles to ./news_2/2024/11/20241122.json\n",
      "Crawling done for 20241122. Progress: 15.56%\n",
      "Saved 18 articles to ./news_2/2024/11/20241123.json\n",
      "Crawling done for 20241123. Progress: 16.67%\n",
      "Saved 26 articles to ./news_2/2024/11/20241124.json\n",
      "Crawling done for 20241124. Progress: 17.78%\n",
      "Saved 165 articles to ./news_2/2024/11/20241125.json\n",
      "Crawling done for 20241125. Progress: 18.89%\n",
      "Saved 143 articles to ./news_2/2024/11/20241126.json\n",
      "Crawling done for 20241126. Progress: 20.00%\n",
      "Saved 123 articles to ./news_2/2024/11/20241127.json\n",
      "Crawling done for 20241127. Progress: 21.11%\n",
      "Saved 100 articles to ./news_2/2024/11/20241128.json\n",
      "Crawling done for 20241128. Progress: 22.22%\n",
      "Saved 141 articles to ./news_2/2024/11/20241129.json\n",
      "Crawling done for 20241129. Progress: 23.33%\n",
      "Saved 18 articles to ./news_2/2024/11/20241130.json\n",
      "Crawling done for 20241130. Progress: 24.44%\n",
      "Saved 18 articles to ./news_2/2024/12/20241201.json\n",
      "Crawling done for 20241201. Progress: 25.56%\n",
      "Saved 151 articles to ./news_2/2024/12/20241202.json\n",
      "Crawling done for 20241202. Progress: 26.67%\n",
      "Saved 186 articles to ./news_2/2024/12/20241203.json\n",
      "Crawling done for 20241203. Progress: 27.78%\n",
      "Saved 283 articles to ./news_2/2024/12/20241204.json\n",
      "Crawling done for 20241204. Progress: 28.89%\n",
      "Saved 182 articles to ./news_2/2024/12/20241205.json\n",
      "Crawling done for 20241205. Progress: 30.00%\n",
      "Saved 186 articles to ./news_2/2024/12/20241206.json\n",
      "Crawling done for 20241206. Progress: 31.11%\n",
      "Saved 19 articles to ./news_2/2024/12/20241207.json\n",
      "Crawling done for 20241207. Progress: 32.22%\n",
      "Saved 48 articles to ./news_2/2024/12/20241208.json\n",
      "Crawling done for 20241208. Progress: 33.33%\n",
      "Saved 238 articles to ./news_2/2024/12/20241209.json\n",
      "Crawling done for 20241209. Progress: 34.44%\n",
      "Saved 222 articles to ./news_2/2024/12/20241210.json\n",
      "Crawling done for 20241210. Progress: 35.56%\n",
      "Saved 181 articles to ./news_2/2024/12/20241211.json\n",
      "Crawling done for 20241211. Progress: 36.67%\n",
      "Saved 192 articles to ./news_2/2024/12/20241212.json\n",
      "Crawling done for 20241212. Progress: 37.78%\n",
      "Saved 168 articles to ./news_2/2024/12/20241213.json\n",
      "Crawling done for 20241213. Progress: 38.89%\n",
      "Saved 20 articles to ./news_2/2024/12/20241214.json\n",
      "Crawling done for 20241214. Progress: 40.00%\n",
      "Saved 49 articles to ./news_2/2024/12/20241215.json\n",
      "Crawling done for 20241215. Progress: 41.11%\n",
      "Saved 168 articles to ./news_2/2024/12/20241216.json\n",
      "Crawling done for 20241216. Progress: 42.22%\n",
      "Saved 168 articles to ./news_2/2024/12/20241217.json\n",
      "Crawling done for 20241217. Progress: 43.33%\n",
      "Saved 175 articles to ./news_2/2024/12/20241218.json\n",
      "Crawling done for 20241218. Progress: 44.44%\n",
      "Saved 222 articles to ./news_2/2024/12/20241219.json\n",
      "Crawling done for 20241219. Progress: 45.56%\n",
      "Saved 154 articles to ./news_2/2024/12/20241220.json\n",
      "Crawling done for 20241220. Progress: 46.67%\n",
      "Saved 19 articles to ./news_2/2024/12/20241221.json\n",
      "Crawling done for 20241221. Progress: 47.78%\n",
      "Saved 32 articles to ./news_2/2024/12/20241222.json\n",
      "Crawling done for 20241222. Progress: 48.89%\n",
      "Saved 174 articles to ./news_2/2024/12/20241223.json\n",
      "Crawling done for 20241223. Progress: 50.00%\n",
      "Saved 139 articles to ./news_2/2024/12/20241224.json\n",
      "Crawling done for 20241224. Progress: 51.11%\n",
      "Saved 30 articles to ./news_2/2024/12/20241225.json\n",
      "Crawling done for 20241225. Progress: 52.22%\n",
      "Saved 165 articles to ./news_2/2024/12/20241226.json\n",
      "Crawling done for 20241226. Progress: 53.33%\n",
      "Saved 203 articles to ./news_2/2024/12/20241227.json\n",
      "Crawling done for 20241227. Progress: 54.44%\n",
      "Saved 20 articles to ./news_2/2024/12/20241228.json\n",
      "Crawling done for 20241228. Progress: 55.56%\n",
      "Saved 37 articles to ./news_2/2024/12/20241229.json\n",
      "Crawling done for 20241229. Progress: 56.67%\n",
      "Saved 203 articles to ./news_2/2024/12/20241230.json\n",
      "Crawling done for 20241230. Progress: 57.78%\n",
      "Saved 47 articles to ./news_2/2024/12/20241231.json\n",
      "Crawling done for 20241231. Progress: 58.89%\n",
      "Saved 28 articles to ./news_2/2025/01/20250101.json\n",
      "Crawling done for 20250101. Progress: 60.00%\n",
      "Saved 277 articles to ./news_2/2025/01/20250102.json\n",
      "Crawling done for 20250102. Progress: 61.11%\n",
      "Saved 139 articles to ./news_2/2025/01/20250103.json\n",
      "Crawling done for 20250103. Progress: 62.22%\n",
      "Saved 16 articles to ./news_2/2025/01/20250104.json\n",
      "Crawling done for 20250104. Progress: 63.33%\n",
      "Saved 30 articles to ./news_2/2025/01/20250105.json\n",
      "Crawling done for 20250105. Progress: 64.44%\n",
      "Saved 160 articles to ./news_2/2025/01/20250106.json\n",
      "Crawling done for 20250106. Progress: 65.56%\n",
      "Saved 153 articles to ./news_2/2025/01/20250107.json\n",
      "Crawling done for 20250107. Progress: 66.67%\n",
      "Saved 161 articles to ./news_2/2025/01/20250108.json\n",
      "Crawling done for 20250108. Progress: 67.78%\n",
      "Saved 136 articles to ./news_2/2025/01/20250109.json\n",
      "Crawling done for 20250109. Progress: 68.89%\n",
      "Saved 124 articles to ./news_2/2025/01/20250110.json\n",
      "Crawling done for 20250110. Progress: 70.00%\n",
      "Saved 19 articles to ./news_2/2025/01/20250111.json\n",
      "Crawling done for 20250111. Progress: 71.11%\n",
      "Saved 33 articles to ./news_2/2025/01/20250112.json\n",
      "Crawling done for 20250112. Progress: 72.22%\n",
      "Saved 127 articles to ./news_2/2025/01/20250113.json\n",
      "Crawling done for 20250113. Progress: 73.33%\n",
      "Saved 146 articles to ./news_2/2025/01/20250114.json\n",
      "Crawling done for 20250114. Progress: 74.44%\n",
      "Saved 174 articles to ./news_2/2025/01/20250115.json\n",
      "Crawling done for 20250115. Progress: 75.56%\n",
      "Saved 149 articles to ./news_2/2025/01/20250116.json\n",
      "Crawling done for 20250116. Progress: 76.67%\n",
      "Saved 114 articles to ./news_2/2025/01/20250117.json\n",
      "Crawling done for 20250117. Progress: 77.78%\n",
      "Saved 18 articles to ./news_2/2025/01/20250118.json\n",
      "Crawling done for 20250118. Progress: 78.89%\n",
      "Saved 40 articles to ./news_2/2025/01/20250119.json\n",
      "Crawling done for 20250119. Progress: 80.00%\n",
      "Saved 159 articles to ./news_2/2025/01/20250120.json\n",
      "Crawling done for 20250120. Progress: 81.11%\n",
      "Saved 192 articles to ./news_2/2025/01/20250121.json\n",
      "Crawling done for 20250121. Progress: 82.22%\n",
      "Saved 171 articles to ./news_2/2025/01/20250122.json\n",
      "Crawling done for 20250122. Progress: 83.33%\n",
      "Saved 171 articles to ./news_2/2025/01/20250123.json\n",
      "Crawling done for 20250123. Progress: 84.44%\n",
      "Saved 107 articles to ./news_2/2025/01/20250124.json\n",
      "Crawling done for 20250124. Progress: 85.56%\n",
      "Saved 19 articles to ./news_2/2025/01/20250125.json\n",
      "Crawling done for 20250125. Progress: 86.67%\n",
      "Saved 23 articles to ./news_2/2025/01/20250126.json\n",
      "Crawling done for 20250126. Progress: 87.78%\n",
      "Saved 17 articles to ./news_2/2025/01/20250127.json\n",
      "Crawling done for 20250127. Progress: 88.89%\n",
      "Saved 19 articles to ./news_2/2025/01/20250128.json\n",
      "Crawling done for 20250128. Progress: 90.00%\n",
      "Saved 19 articles to ./news_2/2025/01/20250129.json\n",
      "Crawling done for 20250129. Progress: 91.11%\n",
      "Saved 31 articles to ./news_2/2025/01/20250130.json\n",
      "Crawling done for 20250130. Progress: 92.22%\n",
      "Saved 150 articles to ./news_2/2025/01/20250131.json\n",
      "Crawling done for 20250131. Progress: 93.33%\n",
      "Saved 16 articles to ./news_2/2025/02/20250201.json\n",
      "Crawling done for 20250201. Progress: 94.44%\n",
      "Saved 36 articles to ./news_2/2025/02/20250202.json\n",
      "Crawling done for 20250202. Progress: 95.56%\n",
      "Saved 206 articles to ./news_2/2025/02/20250203.json\n",
      "Crawling done for 20250203. Progress: 96.67%\n",
      "Saved 159 articles to ./news_2/2025/02/20250204.json\n",
      "Crawling done for 20250204. Progress: 97.78%\n",
      "Saved 184 articles to ./news_2/2025/02/20250205.json\n",
      "Crawling done for 20250205. Progress: 98.89%\n",
      "Saved 201 articles to ./news_2/2025/02/20250206.json\n",
      "Crawling done for 20250206. Progress: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=401&date={target_date}&page={target_page_num}\"\n",
    "start_date = datetime(2024, 11, 9)\n",
    "end_date = datetime(2025, 2, 6)\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_articles_to_file(articles, path):\n",
    "    with open(path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def fetch_url(url, retries=3, delay=0.3):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL {url}: {e}. Retrying ({attempt + 1}/{retries})...\")\n",
    "            attempt += 1\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "def fetch_articles_for_date(target_date):\n",
    "    articles = []\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        url = BASE_URL.format(target_date=target_date, target_page_num=page_num)\n",
    "        response = fetch_url(url)\n",
    "\n",
    "        if response is None:\n",
    "            print(f\"Failed to fetch articles for date {target_date} after multiple attempts.\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        news_list = soup.select('li.newsList > dl > dd > a')\n",
    "\n",
    "        if not news_list:\n",
    "            break\n",
    "\n",
    "        for news in news_list:\n",
    "            news_url = news['href']\n",
    "            pattern = r'article_id=(\\d+)&office_id=(\\d+)'\n",
    "            match = re.search(pattern, news_url)\n",
    "\n",
    "            if match:\n",
    "                article_id = match.group(1)\n",
    "                office_id = match.group(2)\n",
    "                article_url = f'https://n.news.naver.com/mnews/article/{office_id}/{article_id}'\n",
    "                article_response = fetch_url(article_url)\n",
    "\n",
    "                if article_response is None:\n",
    "                    print(f\"Failed to fetch article {article_url} after multiple attempts.\")\n",
    "                    continue\n",
    "\n",
    "                article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "                try:\n",
    "                    title = article_soup.find('h2', class_='media_end_head_headline').text.strip()\n",
    "                except AttributeError:\n",
    "                    title = '정보를 찾을 수 없음'\n",
    "\n",
    "                try:\n",
    "                    date = article_soup.find('span', class_='media_end_head_info_datestamp_time _ARTICLE_DATE_TIME').text.strip()\n",
    "                except AttributeError:\n",
    "                    date = '정보를 찾을 수 없음'\n",
    "\n",
    "                try:\n",
    "                    content = article_soup.find('article', class_='_article_content').text.strip()\n",
    "                except AttributeError:\n",
    "                    content = '정보를 찾을 수 없음'\n",
    "\n",
    "                article_data = {\n",
    "                    'title': title,\n",
    "                    'date': date,\n",
    "                    'content': content\n",
    "                }\n",
    "\n",
    "                articles.append(article_data)\n",
    "\n",
    "        page_num += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "def crawl():\n",
    "    current_date = start_date\n",
    "\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    completed_days = 0\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.strftime('%Y')\n",
    "        month = current_date.strftime('%m')\n",
    "        day = current_date.strftime('%d')\n",
    "\n",
    "        directory_path = f'./news_2/{year}/{month}'\n",
    "        create_directory(directory_path)\n",
    "\n",
    "        target_date = current_date.strftime('%Y%m%d')\n",
    "        articles = fetch_articles_for_date(target_date)\n",
    "\n",
    "        if articles:\n",
    "            file_path = f'{directory_path}/{target_date}.json'\n",
    "            save_articles_to_file(articles, file_path)\n",
    "            print(f\"Saved {len(articles)} articles to {file_path}\")\n",
    "        else:\n",
    "            print(f\"No articles found for date {target_date}\")\n",
    "\n",
    "        completed_days += 1\n",
    "        progress = (completed_days / total_days) * 100\n",
    "        print(f\"Crawling done for {target_date}. Progress: {progress:.2f}%\")\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "# 실행\n",
    "crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykrx import stock\n",
    "\n",
    "def get_stock_name(ticker):\n",
    "    # 주어진 티커의 종목명 조회\n",
    "    try:\n",
    "        stock_name = stock.get_market_ticker_name(ticker)\n",
    "        return stock_name\n",
    "    except:\n",
    "        return \"티커가 유효하지 않습니다.\"\n",
    "    \n",
    "def get_all_stock_names():\n",
    "    # 모든 티커 리스트 가져오기\n",
    "    tickers = stock.get_market_ticker_list()\n",
    "    \n",
    "    # 각 티커의 종목명을 조회하여 리스트에 저장\n",
    "    stock_names = []\n",
    "    for ticker in tickers:\n",
    "        stock_name = get_stock_name(ticker)\n",
    "        stock_names.append((ticker, stock_name))\n",
    "    \n",
    "    return stock_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_names = get_all_stock_names()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# 종목명을 키로, 티커를 값으로 하는 딕셔너리 생성\n",
    "stock_dict = {name: ticker for ticker, name in stock_names}\n",
    "\n",
    "def add_tickers_to_json(src_folder_path, dest_folder_path):\n",
    "    if not os.path.exists(dest_folder_path):\n",
    "        os.makedirs(dest_folder_path)\n",
    "    \n",
    "    for root, dirs, files in os.walk(src_folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                src_file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(src_file_path, src_folder_path)\n",
    "                dest_file_path = os.path.join(dest_folder_path, relative_path)\n",
    "                \n",
    "                os.makedirs(os.path.dirname(dest_file_path), exist_ok=True)\n",
    "\n",
    "                with open(src_file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # data가 리스트임을 가정하고 처리\n",
    "                for item in data:\n",
    "                    content = item.get(\"content\", \"\")\n",
    "                    tickers = []\n",
    "\n",
    "                    for name, ticker in stock_dict.items():\n",
    "                        if name in content:\n",
    "                            tickers.append(ticker)\n",
    "\n",
    "                    if tickers:\n",
    "                        item[\"ticker\"] = tickers\n",
    "\n",
    "                with open(dest_file_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder_path = './news_temp'\n",
    "dest_folder_path = './news'\n",
    "\n",
    "add_tickers_to_json(src_folder_path, dest_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finTF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
